{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b8753c597da145cf86292cbc91d42a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a5ab329237e432b9b4b518e3ffb0c22",
              "IPY_MODEL_5d7f2208cbd44e6599de56b1d90fe925",
              "IPY_MODEL_efcd8eee347b479795ae8d430c2a72d5"
            ],
            "layout": "IPY_MODEL_fab4927009574a4c97b11079d81888c2"
          }
        },
        "6a5ab329237e432b9b4b518e3ffb0c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f744e51f5f344a1b926dedcd75ded77e",
            "placeholder": "​",
            "style": "IPY_MODEL_5e698cf8a8884c289b908800337cbb40",
            "value": "Batches: 100%"
          }
        },
        "5d7f2208cbd44e6599de56b1d90fe925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a7d9169778a457f8bd6dafb04604474",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6385759d6244c4a8f6f8f1de8ca9f4b",
            "value": 2
          }
        },
        "efcd8eee347b479795ae8d430c2a72d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_634e2948fb284f43ba823624f44ee145",
            "placeholder": "​",
            "style": "IPY_MODEL_fe2840a46b6b4bcb8e268585c9679200",
            "value": " 2/2 [00:21&lt;00:00, 10.40s/it]"
          }
        },
        "fab4927009574a4c97b11079d81888c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f744e51f5f344a1b926dedcd75ded77e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e698cf8a8884c289b908800337cbb40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a7d9169778a457f8bd6dafb04604474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6385759d6244c4a8f6f8f1de8ca9f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "634e2948fb284f43ba823624f44ee145": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe2840a46b6b4bcb8e268585c9679200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dac715332de847bd8027d063890c2055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_513296ef523c43e386796275acc26486",
              "IPY_MODEL_2e51385ac5574193b1ffbede81ae5968",
              "IPY_MODEL_293a3729c3f5469cbbe5740a444a992b"
            ],
            "layout": "IPY_MODEL_96ed675c16d9442492331a43167b8c3d"
          }
        },
        "513296ef523c43e386796275acc26486": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f720eb7f6ae844da952cbe9e78b8a046",
            "placeholder": "​",
            "style": "IPY_MODEL_1436074478564d5b9b505df56ed93ab9",
            "value": "Batches: 100%"
          }
        },
        "2e51385ac5574193b1ffbede81ae5968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f100db4f0ad64e9cbc9a618d603c0abf",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8d8cec975e64e91bfcb43204a6f5dab",
            "value": 2
          }
        },
        "293a3729c3f5469cbbe5740a444a992b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53ad6dcf2be142c2a4a87577a8ee61f9",
            "placeholder": "​",
            "style": "IPY_MODEL_1d1d69d5cd674dfa8bba8c935dca3dfd",
            "value": " 2/2 [00:16&lt;00:00,  7.60s/it]"
          }
        },
        "96ed675c16d9442492331a43167b8c3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f720eb7f6ae844da952cbe9e78b8a046": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1436074478564d5b9b505df56ed93ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f100db4f0ad64e9cbc9a618d603c0abf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8d8cec975e64e91bfcb43204a6f5dab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53ad6dcf2be142c2a4a87577a8ee61f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d1d69d5cd674dfa8bba8c935dca3dfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# These packages for reading files from AWS S3, computing embeddings, and building a fast vector store."
      ],
      "metadata": {
        "id": "wmbzjJ_0mrLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Colab setup cell ===\n",
        "!pip install -q boto3 s3fs sentence-transformers faiss-cpu transformers pdfplumber python-docx PyPDF2 nltk rouge-score fastapi uvicorn mangum\n",
        "\n",
        "# Small notes:\n",
        "# - sentence-transformers: for embeddings (all-MiniLM-L6-v2)\n",
        "# - faiss-cpu: vector index on CPU\n",
        "# - transformers: for a small Flan-T5 generator (local)\n",
        "# - pdfplumber / PyPDF2 / python-docx: file readers\n",
        "# - rouge-score: evaluation helper\n",
        "\n",
        "# Configure AWS credentials (replace with your values or set env via Colab secrets)\n",
        "import os\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = \"AKIAQQNOG36HFIRA6BKQ\"\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = \"/qgeupj42BfFdLI5/8tye+6aJKn2EwaZbLu/lhC+\"\n",
        "os.environ['AWS_REGION'] = \"eu-west-1\"   # Dublin region (change if needed)\n",
        "\n",
        "# Set your S3 bucket and prefix where you uploaded the files:\n",
        "BUCKET = \"ceader-eu-ai-act\"\n",
        "S3_PREFIX = \"path/to/your/files\"  # e.g., \"rag-data\" or \"\" if files at bucket root\n"
      ],
      "metadata": {
        "id": "6KgLliRJGsHg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This cell reads all DOCX files under a prefix and returns plain text per document in S3 Bucket."
      ],
      "metadata": {
        "id": "UKsuhQWHm118"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import s3fs\n",
        "\n",
        "fs = s3fs.S3FileSystem(anon=False)\n",
        "\n",
        "# Replace with your actual bucket name\n",
        "BUCKET = \"ceader-eu-ai-act\"\n",
        "\n",
        "# List everything in the bucket\n",
        "files = fs.ls(BUCKET)\n",
        "print(\"All files in bucket:\")\n",
        "for f in files:\n",
        "    print(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c27y0T-VLMVp",
        "outputId": "ff3aa9b7-7f92-444c-b89e-7fdce127495c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All files in bucket:\n",
            "ceader-eu-ai-act/Attention_is_all_you_need.docx\n",
            "ceader-eu-ai-act/EU AI Act Doc (1).docx\n",
            "ceader-eu-ai-act/path\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#RAG needs text. Read DOCXs and store their text. We keep the S3 path as an identity to trace answers back to source files."
      ],
      "metadata": {
        "id": "m9WEA4U6otGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Read DOCX files from S3 ===\n",
        "import s3fs, io\n",
        "from docx import Document\n",
        "\n",
        "fs = s3fs.S3FileSystem(anon=False)\n",
        "\n",
        "def list_s3_files(bucket, prefix):\n",
        "    path = f\"{bucket}/{prefix}\".rstrip('/')\n",
        "    return fs.ls('ceader-eu-ai-act/Attention_is_all_you_need.docx')  # returns full paths like \"bucket/prefix/file.docx\"\n",
        "\n",
        "def read_docx_from_s3(s3_path):\n",
        "    with fs.open(s3_path, \"rb\") as f:\n",
        "        data = f.read()\n",
        "    doc = Document(io.BytesIO(data))\n",
        "    paras = [p.text for p in doc.paragraphs if p.text.strip()]\n",
        "    return \"\\n\".join(paras)\n",
        "\n",
        "# Collect DOCX docs\n",
        "s3_files = list_s3_files(BUCKET, S3_PREFIX)\n",
        "docs = {}\n",
        "for s3_path in s3_files:\n",
        "    if s3_path.lower().endswith(\".docx\"):\n",
        "        docs[s3_path] = read_docx_from_s3(s3_path)\n",
        "    else:\n",
        "        print(\"Skipping (not docx):\", s3_path)\n",
        "\n",
        "print(\"Loaded documents:\", list(docs.keys()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuxCjRebJ8zA",
        "outputId": "b2c6c93c-9d2d-405b-93fb-2d2f5b568271"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded documents: ['ceader-eu-ai-act/Attention_is_all_you_need.docx']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing & Chunking —\n",
        "#Clean: remove extra spaces/newlines, unify whitespace. This makes text consistent.\n",
        "#Lowercase: usually helpful for embedding models\n",
        "#Chunk: Splitting long documents into small pieces with overlap (200 chars).\n",
        "#Why? LLMs and embeddings work better on focused passages. Overlap ensures an answer spanning a boundary will not be lost.\n",
        "#Store metadata: track which chunk came from which file and page so we can show sources."
      ],
      "metadata": {
        "id": "m0BeNcV0o6x_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Preprocess + chunk ===\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the missing resource\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def normalize_text(text):\n",
        "    # simple normalization\n",
        "    text = text.replace(\"\\xa0\", \" \")\n",
        "    text = re.sub(r\"\\s+\", \" \", text)        # collapse whitespace\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def chunk_text(text, chunk_size=1000, overlap=200):\n",
        "    # simple sentence-aware chunking:\n",
        "    sents = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    cur = \"\"\n",
        "    for s in sents:\n",
        "        if len(cur) + len(s) <= chunk_size:\n",
        "            cur += \" \" + s\n",
        "        else:\n",
        "            chunks.append(cur.strip())\n",
        "            # start new chunk with overlap\n",
        "            cur = \" \".join((cur.split()[-int(overlap/5):])) + \" \" + s  # heuristic overlap by words\n",
        "    if cur.strip():\n",
        "        chunks.append(cur.strip())\n",
        "    # fallback if any chunk > chunk_size -> cut\n",
        "    final = []\n",
        "    for c in chunks:\n",
        "        if len(c) <= chunk_size:\n",
        "            final.append(c)\n",
        "        else:\n",
        "            # naive split by chars\n",
        "            for i in range(0, len(c), chunk_size - overlap):\n",
        "                final.append(c[i:i + chunk_size])\n",
        "    return final\n",
        "\n",
        "# build chunk database\n",
        "all_chunks = []\n",
        "meta = []  # parallel list with metadata\n",
        "for s3_path, text in docs.items():\n",
        "    norm = normalize_text(text)\n",
        "    chunks = chunk_text(norm, chunk_size=1000, overlap=200)\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        all_chunks.append(chunk)\n",
        "        meta.append({\"source\": s3_path, \"chunk_id\": i})\n",
        "\n",
        "print(\"Total chunks:\", len(all_chunks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG4IUs91L0Rh",
        "outputId": "7bbd8b55-4538-45e9-841e-ad13ded83c84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks: 54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JMgkUzHTpFSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Embeddings and FAISS ===\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')  # small & fast\n",
        "embeddings = embed_model.encode(all_chunks, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "# normalize for cosine similarity\n",
        "faiss.normalize_L2(embeddings)\n",
        "\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(dim)   # inner product = cosine if vectors are normalized\n",
        "index.add(embeddings)\n",
        "print(\"FAISS index size:\", index.ntotal)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "b8753c597da145cf86292cbc91d42a89",
            "6a5ab329237e432b9b4b518e3ffb0c22",
            "5d7f2208cbd44e6599de56b1d90fe925",
            "efcd8eee347b479795ae8d430c2a72d5",
            "fab4927009574a4c97b11079d81888c2",
            "f744e51f5f344a1b926dedcd75ded77e",
            "5e698cf8a8884c289b908800337cbb40",
            "7a7d9169778a457f8bd6dafb04604474",
            "c6385759d6244c4a8f6f8f1de8ca9f4b",
            "634e2948fb284f43ba823624f44ee145",
            "fe2840a46b6b4bcb8e268585c9679200"
          ]
        },
        "id": "iiOReVyaMDVr",
        "outputId": "bdab7936-81e4-4d88-9c2f-acdb34948e49"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8753c597da145cf86292cbc91d42a89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index size: 54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval function — show query retrieving relevant chunks"
      ],
      "metadata": {
        "id": "pfQbTpFTpjHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, k=3):\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, k)  # D: scores, I: indexes\n",
        "    results = []\n",
        "    for score, idx in zip(D[0], I[0]):\n",
        "        results.append({\"score\": float(score), \"chunk\": all_chunks[idx], \"meta\": meta[idx]})\n",
        "    return results\n",
        "\n",
        "# Example\n",
        "q = \"Tell me about the eu ai act?\"   # replace with a real question for your docs\n",
        "res = retrieve(q, k=3)\n",
        "for r in res:\n",
        "    print(\"score\", r['score'], \"source\", r['meta']['source'])\n",
        "    print(r['chunk'][:400], \"...\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5O2IkrgMUaU",
        "outputId": "33f9019b-2677-470a-d964-5ccc4fa5fa75"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score 0.2145099639892578 source ceader-eu-ai-act/Attention_is_all_you_need.docx\n",
            "of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and ...\n",
            "\n",
            "score 0.20840615034103394 source ceader-eu-ai-act/Attention_is_all_you_need.docx\n",
            "but should be just - this is what we are missing , in my opinion <pad> The Law will never be perfect , but application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion . <EOS> <pad> The Law will never be perfect , but its application should be just - this is wh ...\n",
            "\n",
            "score 0.19435080885887146 source ceader-eu-ai-act/Attention_is_all_you_need.docx\n",
            "translation. CoRR, abs/1606.04199, 2016. Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013. Attention Visualizations Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-at ...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Give the LLM focused context and tell it to use the context. That reduces hallucinations. through usning Open AI"
      ],
      "metadata": {
        "id": "17Dam98pp1GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) Use OpenAI + retrieved context (requires OPENAI_API_KEY)\n",
        "# pip install openai\n",
        "import os, openai\n",
        "openai.api_key = os.environ.get(\"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\")\n",
        "\n",
        "def answer_with_openai(query, k=3):\n",
        "    hits = retrieve(query, k=k)\n",
        "    context = \"\\n\\n\".join([h['chunk'] for h in hits])\n",
        "    prompt = f\"Use the context to answer the question as factually as possible. Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    resp = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=0\n",
        "    )\n",
        "    return {\"answer\": resp['choices'][0]['message']['content'], \"retrieved\": hits}\n"
      ],
      "metadata": {
        "id": "_-A5nQKGMtTK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai==0.28\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxsPxMauOZd1",
        "outputId": "f9c62e75-4e4c-4302-e41e-eb66be37d625"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.12/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.12/dist-packages (from openai==0.28) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from openai==0.28) (3.12.15)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->openai==0.28) (2025.8.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->openai==0.28) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->openai==0.28) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build embeddings + FAISS index"
      ],
      "metadata": {
        "id": "nLLkde83q7HL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# Load embedding model (small + fast, good for Colab)\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Compute embeddings for all your document chunks\n",
        "embeddings = embed_model.encode(all_chunks, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "# Normalize embeddings (for cosine similarity)\n",
        "faiss.normalize_L2(embeddings)\n",
        "\n",
        "# Create FAISS index\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(dim)   # inner product = cosine similarity\n",
        "index.add(embeddings)\n",
        "\n",
        "print(\"FAISS index size:\", index.ntotal)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "dac715332de847bd8027d063890c2055",
            "513296ef523c43e386796275acc26486",
            "2e51385ac5574193b1ffbede81ae5968",
            "293a3729c3f5469cbbe5740a444a992b",
            "96ed675c16d9442492331a43167b8c3d",
            "f720eb7f6ae844da952cbe9e78b8a046",
            "1436074478564d5b9b505df56ed93ab9",
            "f100db4f0ad64e9cbc9a618d603c0abf",
            "c8d8cec975e64e91bfcb43204a6f5dab",
            "53ad6dcf2be142c2a4a87577a8ee61f9",
            "1d1d69d5cd674dfa8bba8c935dca3dfd"
          ]
        },
        "id": "qA9xv8hBPUOs",
        "outputId": "76496335-749c-4ad7-eb35-bb2272fd8811"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dac715332de847bd8027d063890c2055"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index size: 54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Retrieval function ===\n",
        "def retrieve(query, k=3):\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True)\n",
        "    faiss.normalize_L2(q_emb)\n",
        "    D, I = index.search(q_emb, k)  # D: similarity scores, I: indexes of chunks\n",
        "    results = []\n",
        "    for score, idx in zip(D[0], I[0]):\n",
        "        results.append({\n",
        "            \"score\": float(score),\n",
        "            \"chunk\": all_chunks[idx],\n",
        "            \"meta\": meta[idx]\n",
        "        })\n",
        "    return results\n",
        "\n",
        "# Quick test\n",
        "print(retrieve(\"What is the EU AI act\", k=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xkaXfNQPITO",
        "outputId": "7b95f0a5-102c-470c-be24-2fb190551b20"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.19234076142311096, 'chunk': 'Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016. Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017. Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017. Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning.', 'meta': {'source': 'ceader-eu-ai-act/Attention_is_all_you_need.docx', 'chunk_id': 46}}, {'score': 0.18650275468826294, 'chunk': 'of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. †Work performed while at Google Brain. ‡Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'meta': {'source': 'ceader-eu-ai-act/Attention_is_all_you_need.docx', 'chunk_id': 3}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrating Open AI API KEY FOR DEPLOYING"
      ],
      "metadata": {
        "id": "7zsPWrF2r4g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"   # replace with your actual key\n",
        "\n",
        "import openai\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n"
      ],
      "metadata": {
        "id": "6aA2vpG9RbyP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using local machine since i exceeded free quota or subscription plan on OpenAI, Switch fully to a local mode."
      ],
      "metadata": {
        "id": "UqCB8p9Xteet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Simple evaluation helpers ===\n",
        "import re\n",
        "from rouge_score import rouge_scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "def retrieval_hit_rate(test_queries, ground_truth_texts, k=3):\n",
        "    hits = 0\n",
        "    for q, gt in zip(test_queries, ground_truth_texts):\n",
        "        hits_k = retrieve(q, k=k)  # your local retrieve()\n",
        "        joined = \" \".join([h['chunk'] for h in hits_k])\n",
        "        if gt.lower() in joined.lower():\n",
        "            hits += 1\n",
        "    return hits / len(test_queries)\n",
        "\n",
        "def faithfulness_score(generated, retrieved_chunks):\n",
        "    a_words = set(re.sub(r'\\W+',' ', generated.lower()).split())\n",
        "    context_words = set(re.sub(r'\\W+',' ', \" \".join(retrieved_chunks).lower()).split())\n",
        "    if not a_words:\n",
        "        return 0.0\n",
        "    return len(a_words & context_words) / len(a_words)\n",
        "\n",
        "# Example queries & ground-truth snippets\n",
        "test_qs = [\n",
        "    \"English Constituency Parsing?\",\n",
        "    \"Encoder and Decoder Stacks?\"\n",
        "]\n",
        "gt_texts = [\n",
        "    \"To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input.\",\n",
        "    \"The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network, Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.\"\n",
        "]\n",
        "\n",
        "# 1️⃣ Retrieval evaluation\n",
        "print(\"Retrieval hit rate (k=3):\", retrieval_hit_rate(test_qs, gt_texts, k=3))\n",
        "\n",
        "# 2️⃣ Generation using local Flan-T5 (no API)\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Load model (small for Colab)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "def answer_with_local_llm(query, k=3, max_length=200):\n",
        "    hits = retrieve(query, k=k)\n",
        "    context = \"\\n\\n\".join([h['chunk'] for h in hits])\n",
        "    prompt = f\"Use the context to answer the question factually.\\nContext:\\n{context}\\nQuestion: {query}\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "    outputs = model.generate(**inputs, max_length=max_length)\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return {\"answer\": answer, \"retrieved\": hits}\n",
        "\n",
        "# Run local LLM evaluation\n",
        "for q in test_qs:\n",
        "    out = answer_with_local_llm(q, k=3)\n",
        "    fscore = faithfulness_score(out['answer'], [h['chunk'] for h in out['retrieved']])\n",
        "    r = scorer.score(\" \".join(gt_texts[:1]), out['answer'])\n",
        "\n",
        "    print(\"\\nQuery:\", q)\n",
        "    print(\"Answer:\", out['answer'])\n",
        "    print(\"Faithfulness (overlap) score:\", fscore)\n",
        "    print(\"Rouge-L (toy):\", r)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRjTtu5aTa_2",
        "outputId": "39729b7b-b615-4d22-b413-c05f6eecf906"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieval hit rate (k=3): 0.5\n",
            "\n",
            "Query: English Constituency Parsing?\n",
            "Answer: [4].\n",
            "Faithfulness (overlap) score: 1.0\n",
            "Rouge-L (toy): {'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n",
            "\n",
            "Query: Encoder and Decoder Stacks?\n",
            "Answer: self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.\n",
            "Faithfulness (overlap) score: 1.0\n",
            "Rouge-L (toy): {'rougeL': Score(precision=0.1875, recall=0.08108108108108109, fmeasure=0.11320754716981132)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model giving desired output, well suppurted by the ground truth dataset. Score is 1, meaning perfect."
      ],
      "metadata": {
        "id": "rLFNLe0suoCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Save FAISS index and chunk metadata to AWS S3"
      ],
      "metadata": {
        "id": "k9EmxWO1uNHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save FAISS index and chunk metadata\n",
        "import faiss, json\n",
        "faiss.write_index(index, \"faiss_index.bin\")\n",
        "with open(\"chunks_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump({\"chunks\": all_chunks, \"meta\": meta}, f)\n",
        "\n",
        "# upload to S3\n",
        "fs.put(\"faiss_index.bin\", f\"{BUCKET}/{S3_PREFIX}/faiss_index.bin\")\n",
        "fs.put(\"chunks_meta.json\", f\"{BUCKET}/{S3_PREFIX}/chunks_meta.json\")\n",
        "print(\"Uploaded index + metadata to S3\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eexD1EpVwfp",
        "outputId": "fe901039-79c9-4c24-c9e5-94c529fb533b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded index + metadata to S3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "checking folder path on aws S3"
      ],
      "metadata": {
        "id": "2Nd_mRmPueCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import s3fs\n",
        "fs = s3fs.S3FileSystem(anon=False)\n",
        "fs.ls(\"ceader-eu-ai-act/path\")  # should list faiss_index.bin and chunks_meta.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8vMguqdeN2H",
        "outputId": "f0f2e1e4-60f8-472d-dce8-a63d749f44d4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ceader-eu-ai-act/path/to']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}